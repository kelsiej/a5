{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rand\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data. Print out a few lines of it to inspect it's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in csv\n",
    "df = pd.read_csv(\"myData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up text column\n",
    "txt = df.text\n",
    "ntxt = []\n",
    "for i in txt:\n",
    "    temp = str(i).replace('\\n', '')\n",
    "    temp = temp.replace('\\r', ' ')\n",
    "    ntxt.append(temp)\n",
    "    \n",
    "#replace text column with cleaned version\n",
    "df['text'] = ntxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>lines</th>\n",
       "      <th>pagenr</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>8526</td>\n",
       "      <td>paper-search-for-autonomy</td>\n",
       "      <td>46526</td>\n",
       "      <td>1100</td>\n",
       "      <td>19</td>\n",
       "      <td>In his later writing, Maslow (1968) talks of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11051</th>\n",
       "      <td>11052</td>\n",
       "      <td>webster-early-european-history</td>\n",
       "      <td>1411972</td>\n",
       "      <td>29090</td>\n",
       "      <td>400</td>\n",
       "      <td>the dividing line between Rome and Parthia. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11001</th>\n",
       "      <td>11002</td>\n",
       "      <td>webster-early-european-history</td>\n",
       "      <td>1411972</td>\n",
       "      <td>29090</td>\n",
       "      <td>350</td>\n",
       "      <td>butchery. One of the consuls died fighting bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>4582</td>\n",
       "      <td>cia-world-factbook-1992</td>\n",
       "      <td>2463414</td>\n",
       "      <td>64889</td>\n",
       "      <td>2567</td>\n",
       "      <td>lithium, tin, platinum group metals Land u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7258</th>\n",
       "      <td>7259</td>\n",
       "      <td>milton-paradise-lost</td>\n",
       "      <td>481859</td>\n",
       "      <td>10699</td>\n",
       "      <td>269</td>\n",
       "      <td>And worship him; and in reward to rule  Over h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                            name     size  lines  pagenr  \\\n",
       "8525         8526       paper-search-for-autonomy    46526   1100      19   \n",
       "11051       11052  webster-early-european-history  1411972  29090     400   \n",
       "11001       11002  webster-early-european-history  1411972  29090     350   \n",
       "4581         4582         cia-world-factbook-1992  2463414  64889    2567   \n",
       "7258         7259            milton-paradise-lost   481859  10699     269   \n",
       "\n",
       "                                                    text  \n",
       "8525   In his later writing, Maslow (1968) talks of a...  \n",
       "11051  the dividing line between Rome and Parthia. Th...  \n",
       "11001  butchery. One of the consuls died fighting bra...  \n",
       "4581       lithium, tin, platinum group metals Land u...  \n",
       "7258   And worship him; and in reward to rule  Over h...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample of rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 12924\n",
      "Number of titles: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of texts: %i\" % df.shape[0])\n",
    "print(\"Number of titles: %i\" % len(np.unique(df.name.astype('str'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     int64\n",
       "name          object\n",
       "size           int64\n",
       "lines          int64\n",
       "pagenr         int64\n",
       "text          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data types of columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspect some of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mbalbulus-early-life-charlemagne\u001b[0m\n",
      "Eginhard* that have come down to us are—(1) the Life\n",
      "\u001b[1mbible\u001b[0m\n",
      "waters of the flood. Of clean beasts, and of beasts\n",
      "\u001b[1mchipman-earliest-electromagnetic-instruments\u001b[0m\n",
      "the publication, in separate form, of shorter papers from the\n",
      "\u001b[1meckstein-quintus-claudius\u001b[0m\n",
      "me if I had not seen the fellow, with a\n",
      "\u001b[1mfisher-quaker-colonies\u001b[0m\n",
      "of the despised and outlawed Quakers. There he first began\n",
      "\u001b[1mgallienne-quest-of-golden-girl\u001b[0m\n",
      "looks desolation, falling through the thick-blossoming apple-trees as through the\n",
      "\u001b[1mgordon-quiet-talks-crowned-christ\u001b[0m\n",
      "crowned. Not in any vague far-fetched meaning, but in the\n",
      "\u001b[1mhardy-madding-crowd\u001b[0m\n",
      "Norcombe Hill. Through a spur of this hill ran the\n",
      "\u001b[1minfiltrating-open-systems\u001b[0m\n",
      "password files provide comparable protection. The aim of this article\n",
      "\u001b[1mkant-metaphysical-elements-ethics\u001b[0m\n",
      "that they feel themselves irresistibly forced by it. Dissatisfied at\n",
      "\u001b[1mmilton-paradise-lost\u001b[0m\n",
      "out from Heaven, with all his host Of rebel Angels,\n",
      "\u001b[1mnewsgroup\u001b[0m\n",
      "does), mailers at the mail 'gateways' between networks will have\n",
      "\u001b[1mpaper-compact-hash-tables\u001b[0m\n",
      "equal to $t(K)$, and is unsuccessful otherwise. (See Amble and\n",
      "\u001b[1mpaper-data-compression\u001b[0m\n",
      "but require at least one bit to transmit (Gallagher, 1978).\n",
      "\u001b[1mpaper-logical-implementation-of-arithmetic\u001b[0m\n",
      "data structure (0.30,0.31), this is a hidden representation in much\n",
      "\u001b[1mpaper-programming-by-example\u001b[0m\n",
      "actions are also acted upon so that their consequences are\n",
      "\u001b[1mpaper-search-for-autonomy\u001b[0m\n",
      "viruses are introduced and discussed. .bp 1 .ls2 .sh \"Introduction\"\n",
      "\u001b[1mshakespeare-as-you-like-it\u001b[0m\n",
      "a father begot villains. Wert thou not my brother, I\n",
      "\u001b[1munamuno-tragic-sense-of-life\u001b[0m\n",
      "closely resembled Don Miguel de Unamuno. It is not my\n",
      "\u001b[1mvaneeden-quest\u001b[0m\n",
      "had said _Amen_, he peeped expectantly around the darkening room,\n",
      "\u001b[1mwebster-early-european-history\u001b[0m\n",
      "the curriculum which would permit a year's work in English\n",
      "\u001b[1mwhy-speech-output\u001b[0m\n",
      "no tools other than the telephone, and this gives it\n",
      "\u001b[1mworkshop-proceedings\u001b[0m\n",
      "including difficulties in converting preservation microfilm to digital form. Donald\n"
     ]
    }
   ],
   "source": [
    "for i in df['name'].unique():\n",
    "    \n",
    "    current_text = df.loc[df['name'] == i]\n",
    "    rline = rand.randint(1,9)\n",
    "    content = ' '.join(current_text.iloc[rline]['text'].split()[110:120])\n",
    "  \n",
    "    if re.search('[a-zA-Z]', content):\n",
    "        print('\\033[1m' + current_text.iloc[rline]['name'] + '\\033[0m')\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. List all the text sources listed in variable name. How many di\u001berent texts does the data contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balbulus-early-life-charlemagne' 'beesly-queen-elizabeth' 'bible'\n",
      " 'carroll-alice-wonderland' 'chipman-earliest-electromagnetic-instruments'\n",
      " 'cia-world-factbook-1992' 'eckstein-quintus-claudius'\n",
      " 'fisher-quaker-colonies' 'gallienne-quest-of-golden-girl'\n",
      " 'gordon-quiet-talks-crowned-christ' 'hardy-madding-crowd'\n",
      " 'infiltrating-open-systems' 'kant-metaphysical-elements-ethics'\n",
      " 'karn-snowflakes' 'milton-paradise-lost'\n",
      " 'naval-academy-sound-military-decision' 'newsgroup'\n",
      " 'paper-compact-hash-tables' 'paper-data-compression'\n",
      " 'paper-logical-implementation-of-arithmetic'\n",
      " 'paper-programming-by-example' 'paper-search-for-autonomy'\n",
      " 'selected-polish-tales' 'shakespeare-as-you-like-it'\n",
      " 'unamuno-tragic-sense-of-life' 'vaneeden-quest'\n",
      " 'webster-early-european-history' 'why-speech-output'\n",
      " 'workshop-proceedings']\n",
      "\n",
      "There are 29 total texts\n"
     ]
    }
   ],
   "source": [
    "print(df['name'].unique())\n",
    "print('\\nThere are ' + str(len(df['name'].unique())) + ' total texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_array = []\n",
    "for i in df['name'].unique():\n",
    "    dict_text = []\n",
    "    for j in range(len(df)):\n",
    "        if df.iloc[j]['name'] == i:\n",
    "            dict_text.append(df.iloc[j]['text'])\n",
    "    book_dict = {'title':i, 'text': dict_text}\n",
    "    book_array.append(book_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(book_array[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())  \n",
    "\n",
    "word_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 5909)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>...</th>\n",
       "      <th>⁸⁴</th>\n",
       "      <th>⁸⁵</th>\n",
       "      <th>⁸⁶</th>\n",
       "      <th>⁸⁷</th>\n",
       "      <th>⁸⁸</th>\n",
       "      <th>⁸⁹</th>\n",
       "      <th>⁹²</th>\n",
       "      <th>⁹³</th>\n",
       "      <th>⁹¹</th>\n",
       "      <th>⁹⁰</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5909 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   03  10  100  1000  102  103  104  105  106  107 ...  ⁸⁴  ⁸⁵  ⁸⁶  ⁸⁷  ⁸⁸  \\\n",
       "0   1   0    0     0    0    0    0    0    0    0 ...   0   0   0   0   0   \n",
       "1   0   0    0     0    0    0    0    0    0    0 ...   0   0   0   0   0   \n",
       "2   0   0    0     0    0    0    0    0    0    0 ...   0   0   0   0   0   \n",
       "3   0   0    0     0    0    0    0    0    0    0 ...   0   0   0   0   0   \n",
       "4   0   0    0     0    0    0    0    0    0    0 ...   0   0   0   0   0   \n",
       "\n",
       "   ⁸⁹  ⁹²  ⁹³  ⁹¹  ⁹⁰  \n",
       "0   0   0   0   0   0  \n",
       "1   0   0   0   0   0  \n",
       "2   0   0   0   0   0  \n",
       "3   0   0   0   0   0  \n",
       "4   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 5909 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = pd.DataFrame(word_array)\n",
    "\n",
    "word_df.columns = vectorizer.get_feature_names()[:]\n",
    "\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. create the BOW matrix of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize the vectorizer\n",
    "vectorizer = CountVectorizer(min_df=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the dictionary\n",
    "vectorizer.fit(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ensure you understand what the BOW matrix means. What are rows? What are columns? What\n",
    "are the entries? How can you extract word counts for a single text? How about a given word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the matrix is (192, 5909)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the matrix is ' + str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform your data into the BOW array\n",
    "X = vectorizer.transform(df.text).toarray()\n",
    "# rows are sentences, columns are words\n",
    "# you may prefer to mold these into a (dense array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are the sentences, and the rows are the words, the entries are how many times a word appears in that document. If I were to extract word counts for a single text, it would be like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str(X[1])\n",
      "\n",
      "Giving me the array of the word counts for the second text:\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print('str(X[1])')\n",
    "print('\\nGiving me the array of the word counts for the second text:')\n",
    "print(str(X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for a given word, say 'happy', by creating a dataframe with the column names being the feature names and selecting the column for 'happy', I would get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    }
   ],
   "source": [
    "word_df = pd.DataFrame(X)\n",
    "\n",
    "word_df.columns = vectorizer.get_feature_names()[:]\n",
    "\n",
    "print(word_df.happy.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sum of 264 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. split data into trainig and validation parts.\n",
    "\n",
    "Note that you have to split a) the BOW matrix, and b) names of these books as you have to know\n",
    "which book a BOW belogs to. I suggest to create an index vector of observations and split that\n",
    "index, so you will have an index for training and an index for validation observations. Thereafter\n",
    "you simply pull BOW-s and names according to the training index into the training set, and in a\n",
    "similar fashion with the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = 40\n",
    "nVal = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowIndices = np.arange(len(X))\n",
    "iT, iV = train_test_split(rowIndices, train_size=40, test_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets for texts\n",
    "XTrain = X[iT]\n",
    "XV = X[iV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training and validation sets for text names\n",
    "YTrain = df.name[iT]\n",
    "YV = df.name[iV]\n",
    "\n",
    "current_titles = []\n",
    "for i in YTrain:\n",
    "    current_titles.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now implement cosine similarity between these vectors.\n",
    "\n",
    "Ensure you have read the lecture notes about cosine similarity. Write a function that takes in two\n",
    "vectors, x and y, and returns the corresponding cosine similarity c(x,y). Test if c(x, x) = 1, test\n",
    "also a few other vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    cos_sim = dot(x, y)/(norm(x)*norm(y))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(XTrain[1], XTrain[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3130343045619111"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(XTrain[1], XTrain[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5060812723036752"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(XTrain[16], XTrain[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23678574417337256"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(XTrain[7], XTrain[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement cosine similarity:\n",
    "## do it as a function of two vectors\n",
    "## you have to matrix-multiply the corresponding vectors,\n",
    "## and divide by their (Euclidean) norms.\n",
    "## You may use np.linalg.norm for the norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8. Finally, implement k-NN. I mean implement it yourself, don't use pre-existing libraries. \n",
    "\n",
    "I recommend\n",
    "to do 1-NN \u001c",
    "rst, and when this works to extend it to k-NN. You need an algorithm along these lines:\n",
    "(a) Pick a validation case y (later you will loop over all of these).\n",
    "(b) for each vector in the training set xi, compute the cosine similarity c(y, xi). Store this number,\n",
    "and ensure you know which xi corresponds to each c value.\n",
    "(c) Now order all the cosine similarities you just computed in an increasing order.\n",
    "(d) Pick the k largest c-s (i.e. most similar training texts). These correspond to your k nearest\n",
    "neighbors! Ensure you know which texts these are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0051052977849418806, 'cia-world-factbook-1992'],\n",
       " [0.008057314088397157, 'cia-world-factbook-1992'],\n",
       " [0.016402261043504094, 'cia-world-factbook-1992'],\n",
       " [0.04063659510249283, 'newsgroup'],\n",
       " [0.04145968249039981, 'cia-world-factbook-1992'],\n",
       " [0.06533052580081486, 'cia-world-factbook-1992'],\n",
       " [0.10298573010888745, 'why-speech-output'],\n",
       " [0.11060532647706864, 'cia-world-factbook-1992'],\n",
       " [0.11937285617639992, 'cia-world-factbook-1992'],\n",
       " [0.13722155123693863, 'cia-world-factbook-1992'],\n",
       " [0.1382602259640567, 'newsgroup'],\n",
       " [0.1461295076739323, 'vaneeden-quest'],\n",
       " [0.14645077629343128, 'newsgroup'],\n",
       " [0.14679333812580364, 'cia-world-factbook-1992'],\n",
       " [0.15246164126793135, 'vaneeden-quest'],\n",
       " [0.1637307550251127, 'cia-world-factbook-1992'],\n",
       " [0.17970885078258197, 'newsgroup'],\n",
       " [0.18495862441713312, 'beesly-queen-elizabeth'],\n",
       " [0.20003901260096527, 'bible'],\n",
       " [0.20113458206170765, 'vaneeden-quest'],\n",
       " [0.20316275729503885, 'shakespeare-as-you-like-it'],\n",
       " [0.2041545013425201, 'newsgroup'],\n",
       " [0.20441161220602894, 'hardy-madding-crowd'],\n",
       " [0.20444925750286055, 'bible'],\n",
       " [0.20624034926637522, 'naval-academy-sound-military-decision'],\n",
       " [0.21554755034735712, 'why-speech-output'],\n",
       " [0.21698151830137308, 'why-speech-output'],\n",
       " [0.21935625708001263, 'bible'],\n",
       " [0.23775582917272936, 'unamuno-tragic-sense-of-life'],\n",
       " [0.23984244100543872, 'bible'],\n",
       " [0.2457843048032042, 'workshop-proceedings'],\n",
       " [0.24631903993248117, 'bible'],\n",
       " [0.261037006505236, 'bible'],\n",
       " [0.267638403066119, 'selected-polish-tales'],\n",
       " [0.27045228691874595, 'vaneeden-quest'],\n",
       " [0.27078746481997445, 'webster-early-european-history'],\n",
       " [0.2747091310415285, 'webster-early-european-history'],\n",
       " [0.33405353634347196, 'bible'],\n",
       " [0.3402969037106387, 'bible'],\n",
       " [1.0, 'paper-compact-hash-tables']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick random y to test\n",
    "yVector = XTrain[39]\n",
    "y_cs_array = []\n",
    "\n",
    "#go through XTrain and find cosine similarity between XTrain[i] and chosen y\n",
    "for i in range(len(XTrain)):\n",
    "    y_cs_array.append([(cosine_similarity(yVector, XTrain[i])), current_titles[i]])\n",
    "\n",
    "#sort values from least to greatest cosine similarity -- last value is chosen y\n",
    "y_cs_array.sort()\n",
    "y_cs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Now you know your nearest neighbors. But if your k neighbors do not agree, then what? Organize a\n",
    "majority voting among them so you will learn which text is the most popular among the neighbors.\n",
    "I recommend to create a frequency table and pick the most common text source name based on this.\n",
    "In numpy, you can get the frequency table with .value_counts().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bible'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick random y to test\n",
    "yVector = XTrain[39]\n",
    "y_cs_array = []\n",
    "\n",
    "#go through XTrain and find cosine similarity between XTrain[i] and chosen y\n",
    "for i in range(len(XTrain)):\n",
    "    y_cs_array.append([(cosine_similarity(yVector, XTrain[i])), current_titles[i]])\n",
    "\n",
    "#sort values from least to greatest cosine similarity -- last value is chosen y\n",
    "y_cs_array.sort()\n",
    "y_cs_array\n",
    "\n",
    "def k_votes(k, array):\n",
    "    only_titles = []\n",
    "    for i in range(len(array)):\n",
    "        only_titles.append(array[i][1])    \n",
    "    length = len(only_titles)\n",
    "    result = only_titles[(length-k)-1:length-1]\n",
    "    result = max(set(result), key=result.count)\n",
    "    return result\n",
    "\n",
    "k_votes(5, y_cs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0051052977849418806, 'cia-world-factbook-1992'],\n",
       " [0.008057314088397157, 'cia-world-factbook-1992'],\n",
       " [0.016402261043504094, 'cia-world-factbook-1992'],\n",
       " [0.04063659510249283, 'newsgroup'],\n",
       " [0.04145968249039981, 'cia-world-factbook-1992'],\n",
       " [0.06533052580081486, 'cia-world-factbook-1992'],\n",
       " [0.10298573010888745, 'why-speech-output'],\n",
       " [0.11060532647706864, 'cia-world-factbook-1992'],\n",
       " [0.11937285617639992, 'cia-world-factbook-1992'],\n",
       " [0.13722155123693863, 'cia-world-factbook-1992'],\n",
       " [0.1382602259640567, 'newsgroup'],\n",
       " [0.1461295076739323, 'vaneeden-quest'],\n",
       " [0.14645077629343128, 'newsgroup'],\n",
       " [0.14679333812580364, 'cia-world-factbook-1992'],\n",
       " [0.15246164126793135, 'vaneeden-quest'],\n",
       " [0.1637307550251127, 'cia-world-factbook-1992'],\n",
       " [0.17970885078258197, 'newsgroup'],\n",
       " [0.18495862441713312, 'beesly-queen-elizabeth'],\n",
       " [0.20003901260096527, 'bible'],\n",
       " [0.20113458206170765, 'vaneeden-quest'],\n",
       " [0.20316275729503885, 'shakespeare-as-you-like-it'],\n",
       " [0.2041545013425201, 'newsgroup'],\n",
       " [0.20441161220602894, 'hardy-madding-crowd'],\n",
       " [0.20444925750286055, 'bible'],\n",
       " [0.20624034926637522, 'naval-academy-sound-military-decision'],\n",
       " [0.21554755034735712, 'why-speech-output'],\n",
       " [0.21698151830137308, 'why-speech-output'],\n",
       " [0.21935625708001263, 'bible'],\n",
       " [0.23775582917272936, 'unamuno-tragic-sense-of-life'],\n",
       " [0.23984244100543872, 'bible'],\n",
       " [0.2457843048032042, 'workshop-proceedings'],\n",
       " [0.24631903993248117, 'bible'],\n",
       " [0.261037006505236, 'bible'],\n",
       " [0.267638403066119, 'selected-polish-tales'],\n",
       " [0.27045228691874595, 'vaneeden-quest'],\n",
       " [0.27078746481997445, 'webster-early-european-history'],\n",
       " [0.2747091310415285, 'webster-early-european-history'],\n",
       " [0.33405353634347196, 'bible'],\n",
       " [0.3402969037106387, 'bible'],\n",
       " [1.0, 'paper-compact-hash-tables']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10. Compute accuracy (percentage of correct predictions). How good is your algorithm?\n",
    "Now it is time to start increasing the training and testing sets. I repeat: go slow. Training set of 1000\n",
    "and testing set 100 should be \u001c",
    "ne. Go further if your memory and speed permit. When you have found\n",
    "the limits you don't want to exceed (say, 5 mins for the run), it is time to play with k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current training test size, an accuracy score of .3 is recieved when there are 5 nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be used in the following predictions array, just returns an array of the predicted titles\n",
    "def XTrain_cs_array(x):\n",
    "    empty = []\n",
    "    for i in range(len(x)):\n",
    "        yVector = x[i]\n",
    "        y_cs_array = []        \n",
    "        for i in range(len(x)):\n",
    "            y_cs_array.append([(cosine_similarity(yVector, x[i])), current_titles[i]])   \n",
    "        y_cs_array.sort()\n",
    "        empty.append(y_cs_array)       \n",
    "    return empty\n",
    "\n",
    "#accepts the training set and k values, returns the prediction accuracy\n",
    "def predictions(x, k):\n",
    "    cs_values = XTrain_cs_array(x)\n",
    "    predicted_titles = []\n",
    "    for i in range(len(cs_values)):\n",
    "        a = cs_values[i]\n",
    "        b = k_votes(k, a)\n",
    "        predicted_titles.append(b)\n",
    "    score = 0\n",
    "    for i in range(len(current_titles)):   \n",
    "        if current_titles[i] == predicted_titles[i]:\n",
    "            score += 1   \n",
    "    return score / len(current_titles)\n",
    "\n",
    "predictions(XTrain, 5)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the training set to 500 and the testing to 100..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = 500\n",
    "nVal = 100\n",
    "\n",
    "rowIndices = np.arange(len(X))\n",
    "iT, iV = train_test_split(rowIndices, train_size=nTrain, test_size=nVal)\n",
    "\n",
    "# Creating training and validation sets for texts\n",
    "XTrain = X[iT]\n",
    "XV = X[iV]\n",
    "\n",
    "    \n",
    "# Creating training and validation sets for text names\n",
    "YTrain = df.name[iT]\n",
    "YV = df.name[iV]\n",
    "\n",
    "current_titles = []\n",
    "for i in YTrain:\n",
    "    current_titles.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.468"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions(XTrain, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy increased to .468\n",
    "\n",
    "Upping the training set to 1000 and testing to 200..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = 1000\n",
    "nVal = 200\n",
    "\n",
    "rowIndices = np.arange(len(X))\n",
    "iT, iV = train_test_split(rowIndices, train_size=nTrain, test_size=nVal)\n",
    "\n",
    "# Creating training and validation sets for texts\n",
    "XTrain = X[iT]\n",
    "XV = X[iV]\n",
    "\n",
    "    \n",
    "# Creating training and validation sets for text names\n",
    "YTrain = df.name[iT]\n",
    "YV = df.name[iV]\n",
    "\n",
    "current_titles = []\n",
    "for i in YTrain:\n",
    "    current_titles.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.517"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions(XTrain, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the predictions accuracy increased, this time to .517. \n",
    "\n",
    "I would up the sets more, but my computer can't handle it well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. compare different k values. 1, 5, 25 are a good choice. Which k gives you the best performance?\n",
    "What is your highest accuracy?\n",
    "\n",
    "k = 5 was the previous value and gave us .517\n",
    "\n",
    "When k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.528"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions(XTrain, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When k = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.466"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions(XTrain, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a k value of 1 gave the best results, just a bit over k = 5. A k value of 25 gave the worst results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement TF-IDF.\n",
    "## the tf part is just 1 + log-ing the counts\n",
    "## the idf part is slighly more complex: you need counts of all words\n",
    "## use something like X.sum(axis = 0) on numpy array to sum the\n",
    "## elements columnwise.  Now you can easily calculate the\n",
    "## IDF transform of the counts.\n",
    "##\n",
    "## Finally, ensure that tf and idf are multiplied correctly:\n",
    "## the former is a matrix, the latter a vector.  You have to multiply\n",
    "## row-wise, this is something numpy does authomatically but\n",
    "## R does it column-wise, so you have to transpose, multiply, and transpose back.\n",
    "## So you have to ensure your product looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
